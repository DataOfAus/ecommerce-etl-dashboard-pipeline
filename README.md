# ğŸ›’ End-to-End Data Pipeline for E-Commerce Insights

ğŸ“ Project Type: Portfolio / Freelance Showcase

ğŸ’¼ Use Case: Shopify-style e-commerce product pipeline

ğŸ§© Modules: API or CSV ingest â†’ Transform â†’ PostgreSQL â†’ SQL Model â†’ Dashboard

A complete, modular ETL pipeline built with Python, SQL, and PostgreSQL â€” designed to simulate a real-world e-commerce data workflow and deliver analytics-ready outputs for dashboards and decision-making. 

**Delivers a pipeline proven to drastically reduce manual data processing time.**

---

## ğŸš€ Overview

This project demonstrates a full-stack data engineering pipeline from raw data extraction to dashboard-ready insights.

**Designed to solve real-world e-commerce business needs** â€” ideal for freelance professionals, small businesses, and data teams seeking actionable insights. It mimics a Shopify-style data workflow with automated ingestion, data cleaning, transformation, and loading into a PostgreSQL database â€” followed by SQL modeling and a dashboard in Looker Studio.

âš¡ **6.34 seconds** (50k rows) â€” ***99.97% faster** than manual processing*

---
High-level ETL flowchart â€” *see below for performance results*

![Flowchart](docs/images/etl_pipeline_flowchart.png)

---

## ğŸ§± Tech Stack

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Database-blue)
![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-ORM-yellow)
![Looker Studio](https://img.shields.io/badge/Looker%20Studio-Dashboarding-brightgreen)
![Modular ETL](https://img.shields.io/badge/ETL-Modular-orange)

---

## ğŸ“Š Dashboard Output

Interactive dashboard created in **Looker Studio** from modelled PostgreSQL tables. Includes:

- ğŸ“ˆ Product KPIs
- ğŸ›ï¸ Category breakdowns
- ğŸ“Š Performance insights

![Dashboard](docs/images/product_insights_dashboard.png)

---

## ğŸ“ Folder Structure

```bash
ecommerce-etl-dashboard-pipeline/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py                          # Extracts data from API or CSV
â”‚   â”œâ”€â”€ transform.py                        # Cleans & validates data, adds timestamps
â”‚   â”œâ”€â”€ load.py                             # Loads data into PostgreSQL (with retries)
â”‚   â””â”€â”€ __init__.py                         # Makes 'etl' a package
â”œâ”€â”€ tests/                                  # Contains scripts for performance testing
â”‚   â”œâ”€â”€ generate_data.py                    # Generates 50k lines of product data
â”‚   â”œâ”€â”€ test_performance.py                 # Tests same pipeline but using 50k rows of data
â”‚   â””â”€â”€ __init__.py                         # Makes 'tests' a package
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ etl_pipeline_flowchart.png      # High-level project flowchart
â”‚   â”‚   â”œâ”€â”€ product_insights_dashboard.png  # Image of Looker Studio dashboard
â”‚   â”‚   â””â”€â”€ validation/
â”‚   â”‚       â”œâ”€â”€ performance_test.png        # Image of etl test result for 50k rows
â”‚   â”‚       â”œâ”€â”€ row_count.png               # Confirmation of all rows loaded to PostgreSQL
â”‚   â”‚       â””â”€â”€ format_check.png            # Confirmation of format of loaded data
â”‚   â””â”€â”€ sql/
â”‚       â”œâ”€â”€ products_dashboard_ready.sql    # SQL model for dashboard tables
â”‚       â””â”€â”€ validation_queries.sql          # SQL for confirming row count and format
â”œâ”€â”€ .env.example                            # Example config file for DB credentials
â”œâ”€â”€ .gitignore                              # Excludes .env and other sensitive files
â”œâ”€â”€ README.md                               # Project overview (this file)
â””â”€â”€ requirements.txt                        # List of dependencies used by the scripts

```
---

## ğŸ› ï¸ Setup Instructions

1. **Clone the repository**

```bash
git clone https://github.com/DataOfAus/ecommerce-etl-dashboard-pipeline.git
cd ecommerce-etl-dashboard-pipeline
```

2. **Create a `.env` file**

Use the provided template:

```bash
cp .env.example .env
```

Update the `.env` file with your PostgreSQL credentials:
(If youâ€™re unsure of your database credentials, ask your developer or hosting provider)

```
CLOUD_DB_USER=your_username
CLOUD_DB_PASSWORD=your_password
CLOUD_DB_HOST=your_host
CLOUD_DB_PORT=5432
CLOUD_DB_NAME=your_database
```

3. **Install dependencies**

It's recommended to use a virtual environment:

```bash
pip install -r requirements.txt
```

4. **Run the full ETL pipeline**

The `load.py` script automatically calls the extract and transform scripts:

```bash
python load.py
```

This will:
- Extract data (via `extract.py`)
- Transform it (via `transform.py`)
- Load it into PostgreSQL (`load.py`)

5. **Run SQL model to create dashboard table**

Connect to your PostgreSQL database and run the SQL script:

```sql
-- In your SQL client
\i docs/sql/products_dashboard_ready.sql
```

6. **Connect to Looker Studio (optional)**

- Connect Looker Studio to your PostgreSQL database.
- Use the modelled table (`products_dashboard_ready`) to build charts.

---

## â±ï¸  Performance Testing  
*Validating pipeline efficiency at scale*

This project includes tools to test ETL performance with large datasets:
- **`generate_data.py`**: Creates mock Shopify-style product data (50k records)
- **`performance_test.py`**: Times full ETL execution from generation â†’ Neon PostgreSQL load
- **SQL validation scripts**: Verify record counts and data format

### ğŸ” Benchmark Results (50k Records)  
| **Process**               | **Time**  | **Key Optimization**   |
|---------------------------|-----------|------------------------|
| Data Generation           | 0.8s      | Vectorized Pandas      |
| Transformation            | 1.2s      | Parallelized ops       |
| Cloud Load (Neon)         | 4.3s      | SQLAlchemy bulk insert |
| **Total**                 | **6.34s** | **End-to-end**         |

*Compared to a manual workflow*

| End-to-End ETL    | Time              |
|-------------------|-------------------|
| Manually          | ~ 6 hours         | 
| Automated         | **6.34 seconds**  | 

*(Time will scale linearly with larger datasets, depending on API or CSV latency)*

**Performance test yielding the 6.34s total runtime result for 50k records**

![Performance Test Result](docs/images/validation/performance_test.png)

**PostgreSQL query confirming all 50k records loaded**

![Data Validation](docs/images/validation/row_count.png)

*Tested on 4 vCPU, 12 GB RAM (Ubuntu 24.04 LTS VM) with PostgreSQL 16 (Neon.tech)*

---

### ğŸ”„ How to Reproduce  
1. Run the performance test:
```bash
   python tests/test_performance.py
```

---

## ğŸ“ Letâ€™s Build Your Data Solution 

**I help e-commerce teams:** 

âœ… Replace 6-hour manual reports â†’ **6-second pipelines**  
âœ… Turn API chaos â†’ **auto-refreshing dashboards**  
âœ… Guarantee **zero data loss** with industrial validation  

**Why choose me?** 

- ğŸ› ï¸ Professional Engineer â€“ pipelines built for scale  
- ğŸ“Š Full-stack ownership â€“ API to Looker Studio  
- ğŸ“ˆ Proven efficiency â€“ 7,887 rows processed per second in benchmarks
- ğŸ” Full transparency â€“ review the code yourself

Get started:

ğŸ“§ Email: data.of.aus@outlook.com

ğŸ”— LinkedIn: [LinkedIn](https://www.linkedin.com/in/aaron-mietzel/)

---
